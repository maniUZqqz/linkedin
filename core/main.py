import asyncio
import json
import logging
import random
import time
from queue import Queue
from abc import ABC, abstractmethod
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import NoSuchElementException
import pyodbc
import datetime
import requests
import json
import pyodbc
import queue


# Ù„Ø§Ú¯ÛŒÙ† Ú©Ø±Ø¯Ù† ØªÙˆ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

database_project = "Driver={ODBC Driver 17 for SQL Server};Server=.;Database=LinkedInDB;UID=sa;PWD=mani1386;"


# ======================================================================
# Ù…Ø±Ø­Ù„Ù‡ Û±: ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ Ù¾Ø§ÛŒÙ‡ ExtractionStrategy Ùˆ Û· Ú©Ù„Ø§Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
# ======================================================================


# Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡ ÙˆØ¨
class ScrollManager:
    def __init__(self, driver, max_retries=3, scroll_pause=1.5, timeout=30):
        self.driver = driver
        self.max_retries = max_retries
        self.scroll_pause = scroll_pause
        self.timeout = timeout
        self.last_height = 0

    def _scroll_step(self):
        """Ø§Ù†Ø¬Ø§Ù… ÛŒÚ© Ù…Ø±Ø­Ù„Ù‡ Ø§Ø³Ú©Ø±ÙˆÙ„ Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÙˆØ¶Ø¹ÛŒØª ØªØºÛŒÛŒØ± Ø§Ø±ØªÙØ§Ø¹"""
        new_height = self.driver.execute_script(
            "window.scrollTo(0, document.body.scrollHeight);"
            "return document.body.scrollHeight;"
        )
        return new_height != self.last_height, new_height

    def _wait_for_content_load(self):
        """Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø³Ú©Ø±ÙˆÙ„"""
        try:
            WebDriverWait(self.driver, self.scroll_pause).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".feed-shared-update-v2"))
            )
        except TimeoutException:
            logging.warning("Ù‡ÛŒÚ† Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾Ø³ Ø§Ø² Ø§Ø³Ú©Ø±ÙˆÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯")

    def smart_scroll(self, scroll_limit=None):
        """
        Ø§Ø³Ú©Ø±ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ ØªØ§ Ø§Ù†ØªÙ‡Ø§ÛŒ ØµÙØ­Ù‡ ÛŒØ§ Ø±Ø³ÛŒØ¯Ù† Ø¨Ù‡ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø´Ø®Øµ
        Args:
            scroll_limit (int): Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ú©Ø±ÙˆÙ„ (None Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù…Ø­Ø¯ÙˆØ¯)
        Returns:
            int: ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§Øª Ø§Ø³Ú©Ø±ÙˆÙ„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡
        """
        scroll_count = 0
        retries = 0

        while True:
            if scroll_limit and scroll_count >= scroll_limit:
                break

            try:
                changed, new_height = self._scroll_step()
                if not changed:
                    retries += 1
                    if retries >= self.max_retries:
                        break
                    logging.info(f"ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø§Ø³Ú©Ø±ÙˆÙ„ ({retries}/{self.max_retries})")
                    time.sleep(self.scroll_pause * 2)
                    continue

                self.last_height = new_height
                scroll_count += 1
                retries = 0

                logging.debug(f"Ø§Ø³Ú©Ø±ÙˆÙ„ #{scroll_count} Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. Ø§Ø±ØªÙØ§Ø¹ Ø¬Ø¯ÛŒØ¯: {new_height}")
                self._wait_for_content_load()
                time.sleep(self.scroll_pause)
            except Exception as e:
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø­ÛŒÙ† Ø§Ø³Ú©Ø±ÙˆÙ„: {str(e)}")
                break

        logging.info(f"Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ú©Ø±ÙˆÙ„â€ŒÙ‡Ø§: {scroll_count}")
        return scroll_count

    def scroll_to_element(self, element):
        """Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ø§Ù„Ù…Ø§Ù† Ø®Ø§Øµ Ø¨Ø§ Ø§Ù†ÛŒÙ…ÛŒØ´Ù† Ù†Ø±Ù…"""
        try:
            self.driver.execute_script(
                "arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});",
                element
            )
            return True
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ø§Ù„Ù…Ø§Ù†: {str(e)}")
            return False


# ÙˆØ¸ÛŒÙÙ‡: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² LinkedIn.
class LinkedInAnalyzer:
    API_KEY = "tpsg-xuC2QiWGfKsZWcnTRhjLLtGRXPPias9"
    BASE_URL = "https://api.metisai.ir/openai/v1"
    CONNECTION_STRING = database_project

    # Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø§Ù…Ù¾Øª Ø§Ø®ØªØµØ§ØµÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø®Ø´
    SECTION_SYSTEM_PROMPTS = {
        "profile": "You are an expert in analyzing LinkedIn profile data. Analyze the following profile information and provide improvement suggestions.",
        "activity": "You are an expert in analyzing LinkedIn activity data. Please analyze the activity details and provide at least 5 practical suggestions.",
        "education": "You are an expert in analyzing the education section of a LinkedIn profile. Provide constructive feedback and improvement ideas.",
        "skills": "You are an expert in analyzing LinkedIn skills. Review the provided skills data and suggest enhancements.",
        "publications": "You are an expert in analyzing publications. Analyze the publication details and offer improvement recommendations.",
        "honors": "You are an expert in analyzing honors and awards. Review the provided data and suggest ways to improve the presentation.",
        "languages": "You are an expert in analyzing language proficiency data. Analyze the language section and provide suggestions.",
        "projects": "You are an expert in analyzing project information on LinkedIn. Please review the project details and suggest improvements.",
        "volunteering": "You are an expert in analyzing volunteering experience. Analyze the volunteering data and provide actionable suggestions.",
        "certifications": "You are an expert in analyzing certification data. Please review the certifications and suggest enhancements.",
        "courses": "You are an expert in analyzing courses data. Analyze the course details and provide improvement recommendations.",
        "experience": "You are an expert in analyzing professional experience data. Please analyze the experience section and suggest improvements.",
        "organizations": "You are an expert in analyzing organizational affiliations. Review the provided data and offer suggestions for improvement."
    }

    def __init__(self, target):
        self.target = target
        self.data = None

    def get_linkedin_data(self):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ† Ø§Ø² SQL Server Ø¨Ø±Ø§ÛŒ ØªØ§Ø±Ú¯Øª Ù…Ø´Ø®Øµ.
        Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ LinkedInUserData Ø¯Ø§Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:
          - ProfileData, ActivityData, EducationData, SkillsData, PublicationsData,
            HonorsData, LanguagesData, ProjectsData, VolunteeringData, CertificationsData,
            CoursesData, ExperienceData, OrganizationsData, CreatedAt, UpdatedAt
        """
        conn = None
        try:
            conn = pyodbc.connect(self.CONNECTION_STRING)
            cursor = conn.cursor()
            query = """
            SELECT 
                ProfileData, ActivityData, EducationData, SkillsData, PublicationsData,
                HonorsData, LanguagesData, ProjectsData, VolunteeringData, CertificationsData,
                CoursesData, ExperienceData, OrganizationsData, CreatedAt, UpdatedAt
            FROM LinkedInUserData
            WHERE Target = ?
            """
            cursor.execute(query, (self.target,))
            row = cursor.fetchone()
            if row:
                self.data = {}
                self.data['profile'] = json.loads(row.ProfileData) if row.ProfileData else {}
                self.data['activity'] = json.loads(row.ActivityData) if row.ActivityData else []
                self.data['education'] = json.loads(row.EducationData) if row.EducationData else []
                self.data['skills'] = json.loads(row.SkillsData) if row.SkillsData else []
                self.data['publications'] = json.loads(row.PublicationsData) if row.PublicationsData else []
                self.data['honors'] = json.loads(row.HonorsData) if row.HonorsData else []
                self.data['languages'] = json.loads(row.LanguagesData) if row.LanguagesData else []
                self.data['projects'] = json.loads(row.ProjectsData) if row.ProjectsData else {}
                self.data['volunteering'] = json.loads(row.VolunteeringData) if row.VolunteeringData else {}
                self.data['certifications'] = json.loads(row.CertificationsData) if row.CertificationsData else {}
                self.data['courses'] = json.loads(row.CoursesData) if row.CoursesData else []
                self.data['experience'] = json.loads(row.ExperienceData) if row.ExperienceData else []
                self.data['organizations'] = json.loads(row.OrganizationsData) if row.OrganizationsData else []
                self.data['created_at'] = str(row.CreatedAt)
                self.data['updated_at'] = str(row.UpdatedAt)
                return self.data
            else:
                raise RuntimeError("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ§Ø±Ú¯Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        except Exception as e:
            raise RuntimeError(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø§Ø² SQL Server: {str(e)}")
        finally:
            if conn:
                conn.close()

    def build_full_prompt(self):
        """
        Ø³Ø§Ø®Øª ÛŒÚ© prompt Ø¬Ø§Ù…Ø¹ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÙØ§ÛŒÙ„.
        """
        if self.data is None:
            raise RuntimeError("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù‡Ù†ÙˆØ² Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.")
        prompt = f"""
ğŸ” Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ†:

â–«ï¸ Ù¾Ø±ÙˆÙØ§ÛŒÙ„:
{json.dumps(self.data.get('profile', {}), indent=2, ensure_ascii=False)}

â–«ï¸ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§:
{json.dumps(self.data.get('activity', []), indent=2, ensure_ascii=False)}

â–«ï¸ ØªØ­ØµÛŒÙ„Ø§Øª:
{json.dumps(self.data.get('education', []), indent=2, ensure_ascii=False)}

â–«ï¸ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§:
{json.dumps(self.data.get('skills', []), indent=2, ensure_ascii=False)}

â–«ï¸ Ø§Ù†ØªØ´Ø§Ø±Ø§Øª:
{json.dumps(self.data.get('publications', []), indent=2, ensure_ascii=False)}

â–«ï¸ Ø§ÙØªØ®Ø§Ø±Ø§Øª:
{json.dumps(self.data.get('honors', []), indent=2, ensure_ascii=False)}

â–«ï¸ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§:
{json.dumps(self.data.get('languages', []), indent=2, ensure_ascii=False)}

â–«ï¸ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§:
{json.dumps(self.data.get('projects', {}), indent=2, ensure_ascii=False)}

â–«ï¸ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§ÙˆØ·Ù„Ø¨Ø§Ù†Ù‡:
{json.dumps(self.data.get('volunteering', {}), indent=2, ensure_ascii=False)}

â–«ï¸ Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡â€ŒÙ‡Ø§:
{json.dumps(self.data.get('certifications', {}), indent=2, ensure_ascii=False)}

â–«ï¸ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§:
{json.dumps(self.data.get('courses', []), indent=2, ensure_ascii=False)}

â–«ï¸ ØªØ¬Ø±Ø¨ÛŒØ§Øª Ø´ØºÙ„ÛŒ:
{json.dumps(self.data.get('experience', []), indent=2, ensure_ascii=False)}

â–«ï¸ Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§:
{json.dumps(self.data.get('organizations', []), indent=2, ensure_ascii=False)}

Created At: {self.data.get('created_at')}
Updated At: {self.data.get('updated_at')}
"""
        return prompt

    def send_api_request(self, payload):
        """
        Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API Ù…ØªÛŒØ³ Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø®.
        """
        headers = {
            "Authorization": f"Bearer {self.API_KEY}",
            "Content-Type": "application/json"
        }
        response = requests.post(f"{self.BASE_URL}/chat/completions", json=payload, headers=headers)
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["message"]["content"]
        else:
            return f"Ø®Ø·Ø§ÛŒ API: {response.status_code} - {response.text}"

    def analyze_full(self):
        """
        ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ©Ø¨Ø§Ø±Ù‡.
        """
        if self.data is None:
            self.get_linkedin_data()
        full_prompt = self.build_full_prompt()
        analysis_prompt = f"""
Ø±ÙˆØ¨ÛŒÚ©Ù…Ù¾ÛŒ Ø¹Ø²ÛŒØ² Ù…Ù†ØŒ Ù„Ø·ÙØ§ Ø§ÛŒÙ† Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ† Ø±Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ù† Ùˆ Ø­Ø¯Ø§Ù‚Ù„ 5 Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø¯ÛŒ:

{full_prompt}

Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„:
1. Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Keyword Optimization)
2. Ø³Ø§Ø®ØªØ§Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ (Professional Structure)
3. Ø¬Ø°Ø§Ø¨ÛŒØª Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… (Recruitment Appeal)
4. Ø±Ø¹Ø§ÛŒØª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ù‡Ø§ÛŒ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ† (LinkedIn Standards)
5. Ù†Ú©Ø§Øª ÙÙ†ÛŒ Ùˆ Ø¸Ø§Ù‡Ø±ÛŒ (Technical Aspects)
"""
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": (
                    "You are 'Rubikamp', an intelligent assistant who communicates in Persian. "
                    "Always address the user warmly, provide 5-10 practical solutions, and explain technical terms in both English and Persian."
                )},
                {"role": "user", "content": analysis_prompt}
            ],
            "max_tokens": 1000,
            "temperature": 0.5
        }
        return self.send_api_request(payload)

    def analyze_section(self, section_name, content):
        """
        ØªØ­Ù„ÛŒÙ„ ÛŒÚ© Ø¨Ø®Ø´ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø§Ù…Ù¾Øª Ø§Ø®ØªØµØ§ØµÛŒ.
        """
        system_prompt = self.SECTION_SYSTEM_PROMPTS.get(
            section_name,
            "Please analyze the following data and provide improvement suggestions."
        )
        user_prompt = f"""
Ø±ÙˆØ¨ÛŒÚ©Ù…Ù¾ÛŒ Ø¹Ø²ÛŒØ² Ù…Ù†ØŒ Ù„Ø·ÙØ§ Ø¨Ø®Ø´ {section_name} Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ† Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù† Ùˆ Ø­Ø¯Ø§Ù‚Ù„ 5 Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø±Ø§Ø¦Ù‡ Ø¨Ø¯Ù‡:

{json.dumps(content, indent=2, ensure_ascii=False)}
"""
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": 1000,
            "temperature": 0.5
        }
        return self.send_api_request(payload)

    def analyze_sections_individually(self):
        """
        ØªØ­Ù„ÛŒÙ„ Ù‡Ø± Ø¨Ø®Ø´ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² SQL Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ùˆ Ù‡Ø± Ø¨Ø®Ø´ Ø¯Ø± ÛŒÚ© ØµÙ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
        Ø³Ù¾Ø³ Ù‡Ø± ØªØ³Ú© Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ù‡ API Ù…ØªÛŒØ³ Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        if self.data is None:
            self.get_linkedin_data()
        results = {}
        task_queue = queue.Queue()
        for section in self.SECTION_SYSTEM_PROMPTS.keys():
            content = self.data.get(section)
            if content not in (None, {}, []):
                task_queue.put((section, content))
        while not task_queue.empty():
            section, content = task_queue.get()
            response = self.analyze_section(section, content)
            results[section] = response
            task_queue.task_done()
        return results



# Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
class DuplicateRemover:
    @staticmethod
    def remove_duplicates(records: list, unique_key: str) -> list:
        """
        Ø§Ø² Ø±ÙˆÛŒ ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒÙ‡Ø§ØŒ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù„ÛŒØ¯ unique_key Ø¯Ø§Ø±Ù†Ø¯ Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

        Args:
            records (list): Ù„ÛŒØ³Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒÙ‡Ø§.
            unique_key (str): Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ ÛŒÚ©ØªØ§ Ø¨Ø§Ø´Ø¯ (Ù…Ø«Ù„Ø§Ù‹ "organization_name").

        Returns:
            list: Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† ØªÚ©Ø±Ø§Ø±.
        """
        seen = set()
        deduped_records = []
        for record in records:
            # Ø¯Ø±ÛŒØ§ÙØª Ù…Ù‚Ø¯Ø§Ø± Ú©Ù„ÛŒØ¯ØŒ Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÛŒÚ©Ø³Ø§Ù†
            key_value = record.get(unique_key, "").strip().lower()
            # Ø§Ú¯Ø± Ù…Ù‚Ø¯Ø§Ø± Ú©Ù„ÛŒØ¯ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ Ùˆ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯ÛŒØ¯Ù‡ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
            if key_value and key_value not in seen:
                seen.add(key_value)
                deduped_records.append(record)
        return deduped_records



class ExtractionStrategy(ABC):
    @abstractmethod
    def extract(self, driver):
        pass

    @abstractmethod
    def get_identifier(self):
        pass




class ProfileExtraction(ExtractionStrategy):
    def extract(self, driver):
        try:
            wait = WebDriverWait(driver, 15)
            # ØµØ¨Ø± ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø¨Ø®Ø´ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ (top card) Ù„ÙˆØ¯ Ø´ÙˆØ¯
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".ph5.pb5")))

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ú©Ø³ Ù¾Ø±ÙˆÙØ§ÛŒÙ„
            profile_picture = ""
            try:
                profile_pic_elem = driver.find_element(
                    By.CSS_SELECTOR,
                    "div.pv-top-card__non-self-photo-wrapper button img"
                )
                profile_picture = profile_pic_elem.get_attribute("src")
            except Exception as e:
                logging.debug("Ø¹Ú©Ø³ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù…
            name = ""
            try:
                name_elem = driver.find_element(
                    By.CSS_SELECTOR,
                    "h1.uQioaVkRgsfWDoBSLjlSfqboaPcwugUOAnAo"
                )
                name = name_elem.text.strip()
            except Exception as e:
                logging.debug("Ù†Ø§Ù… ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ (tagline)
            tagline = ""
            try:
                tagline_elem = driver.find_element(
                    By.CSS_SELECTOR,
                    "div.text-body-medium.break-words"
                )
                tagline = tagline_elem.text.strip()
            except Exception as e:
                logging.debug("ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª ÙØ¹Ù„ÛŒ (Ù†Ø§Ù… Ùˆ Ù„ÙˆÚ¯Ùˆ)
            current_company = ""
            company_logo = ""
            try:
                # ÛŒØ§ÙØªÙ† Ø¯Ú©Ù…Ù‡ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø´Ø±Ú©Øª ÙØ¹Ù„ÛŒ Ø§Ø² Ø¯Ø§Ø®Ù„ ul Ø¨Ø§ Ú©Ù„Ø§Ø³ Ù…Ø´Ø®Øµ
                company_button = driver.find_element(
                    By.CSS_SELECTOR,
                    "ul.erRWcxhwyYllLRgdZRauLQBzZMdDNmhPoE li button"
                )
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÙˆÚ¯ÙˆÛŒ Ø´Ø±Ú©Øª
                try:
                    company_logo_elem = company_button.find_element(By.CSS_SELECTOR, "img")
                    company_logo = company_logo_elem.get_attribute("src")
                except Exception as e:
                    logging.debug("Ù„ÙˆÚ¯ÙˆÛŒ Ø´Ø±Ú©Øª ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø´Ø±Ú©Øª Ø§Ø² Ù…ØªÙ† Ø¯Ø§Ø®Ù„ span (Ø¯Ø§Ø®Ù„ ÛŒÚ© div)
                try:
                    company_name_elem = company_button.find_element(
                        By.CSS_SELECTOR,
                        "span.BsvdPyOtOKNGGVYYQqtUElKbZGPnfpcuzcOc div"
                    )
                    current_company = company_name_elem.text.strip()
                except Exception as e:
                    logging.debug("Ù†Ø§Ù… Ø´Ø±Ú©Øª ÙØ¹Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)
            except Exception as e:
                logging.debug("Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª ÙØ¹Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ (location)
            location = ""
            try:
                location_elem = driver.find_element(
                    By.CSS_SELECTOR,
                    "div.IOuUwFmttqBhtvZnhWkJNpdEtSloAmSnVqMA.mt2 span.text-body-small.inline.t-black--light.break-words"
                )
                location = location_elem.text.strip()
            except Exception as e:
                logging.debug("Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª
            connections = ""
            try:
                connections_elem = driver.find_element(
                    By.CSS_SELECTOR,
                    "ul.CRNhzCZvlJndiJjJyEOFuGmFrPhKyyjecU.NDxUENCcMFlOuVWFBEYHGASScFfhIJtKZbCg li span.t-bold"
                )
                connections = connections_elem.text.strip()
            except Exception as e:
                logging.debug("ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)

            # ---------- Ø¨Ø®Ø´ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ø³Ù…Øª "about" ----------
            # Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ ØªØ§ Ù‚Ø³Ù…ØªÛŒ Ú©Ù‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ø¢ÛŒØ¯
            time.sleep(1)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(1)

            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ú©Ù…Ù‡ Â«â€¦see moreÂ» Ùˆ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ø¢Ù† Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯
            try:
                more_button = driver.find_element(
                    By.CSS_SELECTOR,
                    "button.inline-show-more-text__button.inline-show-more-text__button--light.link"
                )
                # Ø§Ú¯Ø± Ø¯Ú©Ù…Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯ Ùˆ Ù‡Ù†ÙˆØ² Ø¨Ø§Ø² Ù†Ø´Ø¯Ù‡ (aria-expanded Ø¨Ø±Ø§Ø¨Ø± false Ø§Ø³Øª)
                if more_button and more_button.get_attribute("aria-expanded") == "false":
                    driver.execute_script("arguments[0].click();", more_button)
                    time.sleep(1)
            except Exception as e:
                logging.debug("Ø¯Ú©Ù…Ù‡ 'see more' Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ ÛŒØ§ Ù‚Ø§Ø¨Ù„ Ú©Ù„ÛŒÚ© Ù†Ø¨ÙˆØ¯: %s", e)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù‚Ø³Ù…Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ (about)
            about = ""
            try:
                about_elem = driver.find_element(
                    By.CSS_SELECTOR,
                    "div.display-flex.ph5.pv3"
                )
                about = about_elem.text.strip()
            except Exception as e:
                logging.debug("Ù…ØªÙ† Ø¯Ø±Ø¨Ø§Ø±Ù‡ (about) ÛŒØ§ÙØª Ù†Ø´Ø¯: %s", e)
            # ----------------------------------------------------------------

            return {
                "profile_picture": profile_picture,
                "name": name,
                "tagline": tagline,
                "current_company": current_company,
                "company_logo": company_logo,
                "location": location,
                "connections": connections,
                "about": about
            }
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø±ÙˆÙØ§ÛŒÙ„: {str(e)}")
            return None

    def get_identifier(self):
        return "profile"

class ActivityExtraction(ExtractionStrategy):
    def get_identifier(self):
        return "activity"

    def __init__(self):
        self.scroll_manager = None

    def extract(self, driver):
        try:
            self.scroll_manager = ScrollManager(driver)
            self._scroll_to_load_posts()
            posts_data = self._process_posts(driver)
            return posts_data
        except Exception as e:
            logging.critical(f"Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§: {e}", exc_info=True)
            return None
        finally:
            self.scroll_manager = None

    def _scroll_to_load_posts(self):
        """
        Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡ ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ Ø¨Ø±Ø³ÛŒÙ….
        ÙØ±Ø¶ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…ØªØ¯ smart_scroll Ø¯Ø± ScrollManager Ø§ÛŒÙ† Ù…Ù†Ø·Ù‚ Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        logging.info("Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø³Øªâ€ŒÙ‡Ø§")
        scroll_count = self.scroll_manager.smart_scroll()
        logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙØ¹Ø§Øª Ø§Ø³Ú©Ø±ÙˆÙ„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡: {scroll_count}")

    def _process_posts(self, driver):
        """
        ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù…ÛŒ Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙÛŒØ¯ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡.
        """
        try:
            post_elements = WebDriverWait(driver, 20).until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, "div.feed-shared-update-v2:not(.comments-comment-item)")
                )
            )
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§ÙØªÙ† Ù¾Ø³Øªâ€ŒÙ‡Ø§: {e}")
            return {"posts": [], "count": 0}

        posts = []
        for idx, post in enumerate(post_elements, 1):
            posts.append(self._extract_post_data(post, idx))
        return posts

    def _extract_post_data(self, post, index):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ù¾Ø³Øª Ø´Ø§Ù…Ù„ Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…Ø­ØªÙˆØ§ØŒ ØªØ§Ø±ÛŒØ®ØŒ ØªØ¹Ø§Ù…Ù„Ø§Øª Ùˆ Ù…Ø¯ÛŒØ§.
        Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø¬ØŒ ØµÙØ­Ù‡ Ø¨Ù‡ Ù¾Ø³Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø§Ø³Ú©Ø±ÙˆÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        self.scroll_manager.scroll_to_element(post)
        return {
            "post_number": index,
            "title": self._extract_title(post),
            "content": self._extract_content(post),
            "date": self._extract_date(post),
            "engagement": self._extract_engagement(post),
            "media": self._extract_media(post)
        }

    def _safe_extract_text(self, element, by, selector, default=""):
        """
        ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² Ø¹Ù†ØµØ±ÛŒ Ù…Ø´Ø®ØµØ› Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ØŒ Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        try:
            return element.find_element(by, selector).text.strip()
        except Exception as e:
            logging.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ selector '{selector}': {e}")
            return default

    def _click_more_button(self, post):
        """
        Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø¯Ú©Ù…Ù‡ Â«Ù†Ù…Ø§ÛŒØ´ Ø¨ÛŒØ´ØªØ±Â»ØŒ Ø±ÙˆÛŒ Ø¢Ù† Ú©Ù„ÛŒÚ© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.
        """
        try:
            more_button = post.find_element(By.CSS_SELECTOR, 'button[aria-label="Ù†Ù…Ø§ÛŒØ´ Ø¨ÛŒØ´ØªØ±"]')
            self.scroll_manager.driver.execute_script("arguments[0].click();", more_button)
            time.sleep(0.5)
        except Exception as e:
            logging.debug(f"Ø¯Ú©Ù…Ù‡ 'Ù†Ù…Ø§ÛŒØ´ Ø¨ÛŒØ´ØªØ±' ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ù‚Ø§Ø¨Ù„ Ú©Ù„ÛŒÚ© Ù†Ø¨ÙˆØ¯: {e}")

    def _extract_title(self, post):
        return self._safe_extract_text(
            post, By.CSS_SELECTOR, 'span.update-components-text__break-words > strong'
        )

    def _extract_content(self, post):
        self._click_more_button(post)
        return self._safe_extract_text(
            post, By.CSS_SELECTOR, 'div.feed-shared-inline-show-more-text'
        )

    def _extract_date(self, post):
        return self._safe_extract_text(
            post, By.CSS_SELECTOR, 'span.update-components-actor__sub-description > span:not(.visually-hidden)'
        )

    def _extract_engagement(self, post):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø§Ù…Ù„Ø§Øª Ù¾Ø³Øª Ø´Ø§Ù…Ù„ Ù„Ø§ÛŒÚ©â€ŒÙ‡Ø§ØŒ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ Ùˆ Ø±ÛŒÙ¾Ø³Øªâ€ŒÙ‡Ø§.
        """
        return {
            "likes": self._get_likes_count(post),
            "comments": self._get_comments_count(post),
            "reposts": self._get_reposts_count(post)
        }

    def _get_likes_count(self, post):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ Ù„Ø§ÛŒÚ©â€ŒÙ‡Ø§ Ø§Ø² HTML Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡:
        <li class="social-details-social-counts__item ...">
            <button ... aria-label="39 reactions" ...>
                ...
                <span class="social-details-social-counts__reactions-count">39</span>
            </button>
        </li>
        """
        try:
            element = post.find_element(
                By.XPATH,
                ".//li[contains(@class, 'social-details-social-counts__reactions')]//span[contains(@class, 'social-details-social-counts__reactions-count')]"
            )
            text = element.text.strip()
            num_text = text.replace(',', '')
            return int(num_text) if num_text.isdigit() else 0
        except Exception as e:
            logging.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„Ø§ÛŒÚ©â€ŒÙ‡Ø§: {e}")
            return 0

    def _get_comments_count(self, post):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ Ø§Ø² HTML Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡:
        <li class="social-details-social-counts__item social-details-social-counts__comments ...">
            <button aria-label="2 comments" ...>
                <span>2 comments</span>
            </button>
        </li>
        """
        try:
            element = post.find_element(
                By.XPATH,
                ".//li[contains(@class, 'social-details-social-counts__comments')]//button//span"
            )
            text = element.text.strip()  # Ù…Ø«Ù„Ø§Ù‹ "2 comments"
            count_str = text.split()[0]
            num_text = count_str.replace(',', '')
            return int(num_text) if num_text.isdigit() else 0
        except Exception as e:
            logging.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§: {e}")
            return 0

    def _get_reposts_count(self, post):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ Ø±ÛŒÙ¾Ø³Øªâ€ŒÙ‡Ø§ Ø§Ø² HTML Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡:
        <li class="social-details-social-counts__item ...">
            <button aria-label="2 reposts" ...>
                <span>2 reposts</span>
            </button>
        </li>
        """
        try:
            element = post.find_element(
                By.XPATH,
                ".//button[contains(@aria-label, 'reposts')]//span"
            )
            text = element.text.strip()  # Ù…Ø«Ù„Ø§Ù‹ "2 reposts"
            count_str = text.split()[0]
            num_text = count_str.replace(',', '')
            return int(num_text) if num_text.isdigit() else 0
        except Exception as e:
            logging.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÛŒÙ¾Ø³Øªâ€ŒÙ‡Ø§: {e}")
            return 0

    def _extract_media(self, post):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù…ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ù…Ø¯ÛŒØ§ (ÙˆÛŒØ¯Ø¦ÙˆØŒ ØªØµÙˆÛŒØ±ØŒ Ø³Ù†Ø¯) Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù¾Ø³Øª.
        """
        media = []
        media_selectors = [
            'div.feed-shared-linkedin-video',
            'img.feed-shared-image__image',
            'div.feed-shared-document__container'
        ]
        for selector in media_selectors:
            try:
                items = post.find_elements(By.CSS_SELECTOR, selector)
                for item in items:
                    media.append(self._process_media_item(item))
            except Exception as e:
                logging.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¯ÛŒØ§ Ø¨Ø§ selector '{selector}': {e}")
        return media

    def _process_media_item(self, item):
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù…ÙˆØ±Ø¯ Ù…Ø¯ÛŒØ§ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù† Ø´Ø§Ù…Ù„ Ù†ÙˆØ¹ØŒ URL Ùˆ Ù…ØªÙ† Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† (alt text).
        """
        media_type = self._detect_media_type(item)
        url = item.get_attribute("src") or item.get_attribute("href") or ""
        alt_text = (item.get_attribute("alt") or "")[:100]
        return {"type": media_type, "url": url, "alt_text": alt_text}

    def _detect_media_type(self, element):
        """
        ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ù…Ø¯ÛŒØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¹Ù†ØµØ±.
        """
        class_list = element.get_attribute("class") or ""
        if "feed-shared-linkedin-video" in class_list:
            return "video"
        if "feed-shared-image__image" in class_list:
            return "image"
        if "feed-shared-document__container" in class_list:
            return "document"
        return "unknown"

class EducationExtraction(ExtractionStrategy):
    def extract(self, driver):
        try:
            # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø®Ø´ ØªØ­ØµÛŒÙ„Ø§Øª
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div[data-view-name='profile-component-entity']"))
            )

            education_items = driver.find_elements(By.CSS_SELECTOR, "div[data-view-name='profile-component-entity']")
            results = []

            for item in education_items:
                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Â«Ù…Ù‚Ø·Ø¹ ØªØ­ØµÛŒÙ„ÛŒÂ»:
                    # Ø§Ù„Ù…Ø§Ù†ÛŒ Ú©Ù‡ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ t-14 Ùˆ t-normal Ø¨Ø§Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† t-black--light)
                    degree_element = item.find_element(
                        By.XPATH,
                        ".//span[contains(@class, 't-14') and contains(@class, 't-normal') and not(contains(@class, 't-black--light'))]"
                    )
                    degree = degree_element.text.strip()
                except Exception:
                    degree = ""

                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Â«Ø²Ù…Ø§Ù†Â»:
                    # Ø§Ù„Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ t-14ØŒ t-normal Ùˆ t-black--light Ø§Ø³Øª
                    date_element = item.find_element(
                        By.XPATH,
                        ".//span[contains(@class, 't-14') and contains(@class, 't-normal') and contains(@class, 't-black--light')]"
                    )
                    date = date_element.text.strip()
                except Exception:
                    date = ""

                results.append({
                    "degree": degree,
                    "date": date
                })

            return results

        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ­ØµÛŒÙ„Ø§Øª: {str(e)}")
            return None

    def count_items(self, driver):
        """
        Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø¨ØªØ¯Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ­ØµÛŒÙ„ÛŒ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù¾Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        """
        results = self.extract(driver)
        return len(results) if results is not None else 0
    def get_identifier(self):
        return "education"

class SkillsExtraction(ExtractionStrategy):
    def get_identifier(self):
        return "skills"

    def extract(self, driver):
        # Ù…Ø±Ø­Ù„Ù‡ Û±: Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡
        try:
            self._scroll_page(driver)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡: {str(e)}", exc_info=True)
            return None

        # Ù…Ø±Ø­Ù„Ù‡ Û²: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ØµØ± Ù…Ù‡Ø§Ø±Øª
        try:
            skill_elements = self._extract_skill_elements(driver)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ØµØ± Ù…Ù‡Ø§Ø±Øª: {str(e)}", exc_info=True)
            return None

        # Ù…Ø±Ø­Ù„Ù‡ Û³: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§
        try:
            skills = self._process_skills(skill_elements)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§: {str(e)}", exc_info=True)
            return None

        # Ù†Ù…Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
        logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(skills)}")
        return skills

    def _scroll_page(self, driver):
        """
        Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡ ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø¯ÛŒÚ¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ Ù„ÙˆØ¯ Ù†Ø´ÙˆØ¯.
        """
        # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø´Ø¯Ù† Ø¹Ù†ØµØ± body ØµÙØ­Ù‡
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))

        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0

        while True:
            # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # ØªØ§Ø®ÛŒØ± Ø¬Ù‡Øª Ù„ÙˆØ¯ Ø´Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯ÛŒØ¯

            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                scroll_attempts += 1
                if scroll_attempts >= 3:  # Ø¯Ø± ØµÙˆØ±Øª Û³ Ø¨Ø§Ø± ØªÙ„Ø§Ø´ Ù†Ø§Ù…ÙˆÙÙ‚ØŒ ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    break
            else:
                scroll_attempts = 0
                last_height = new_height

    def _extract_skill_elements(self, driver):
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†Ø§ØµØ± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ CSS Selector Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ.
        """
        css_selector = (
            ".pv-skill-category-entity__name-text, "
            "[data-field='skill_page_skill_topic'] .t-bold, "
            ".skill-category-entity__name"
        )
        return WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, css_selector))
        )

    def _process_skills(self, skill_elements):
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¹Ù†Ø§ØµØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª ÛŒÚ© Ù„ÛŒØ³Øª ÛŒÚ©ØªØ§ Ø§Ø² Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§.
        """
        unique_skills = set()
        for element in skill_elements:
            skill_text = element.text.strip()
            if skill_text:
                unique_skills.add(skill_text)
        return list(unique_skills)

class PublicationsExtraction(ExtractionStrategy):
    def get_identifier(self):
        # ÛŒÚ© Ø´Ù†Ø§Ø³Ù‡ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯
        return "publications"

    def extract(self, driver):
        try:
            # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø´Ø¯Ù† Ø¨Ø®Ø´ Ø§Ù†ØªØ´Ø§Ø±Ø§Øª Ø¨Ø§ XPath Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.XPATH, '//section[contains(*,"Publications")]'))
            )

            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ScrollManager Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯
            scroll_manager = ScrollManager(driver)
            scroll_manager.smart_scroll()

            publications_data = []

            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ´Ø§Ø±
            publications = driver.find_elements(
                By.XPATH,
                '//section[contains(*,"Publications")]//div[contains(@class, "pvs-list__container")]//li'
            )

            for pub in publications:
                try:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ
                    title = pub.find_element(
                        By.XPATH,
                        './/div[contains(@class, "t-bold")]//span[contains(@aria-hidden, "true")]'
                    ).text.strip()

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø²Ø¦ÛŒØ§Øª (Ù…Ø§Ù†Ù†Ø¯ Ù…Ø¬Ù„Ù‡ Ùˆ ØªØ§Ø±ÛŒØ®)
                    details = pub.find_element(
                        By.XPATH,
                        './/span[contains(@class, "t-normal")]//span[contains(@aria-hidden, "true")]'
                    ).text.strip()

                    publications_data.append({
                        "title": title,
                        "details": details
                    })
                except Exception as e:
                    logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢ÛŒØªÙ…: {str(e)}")

            logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ù†ØªØ´Ø§Ø±Ø§Øª: {len(publications_data)}")
            return publications_data

        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù†ØªØ´Ø§Ø±Ø§Øª: {str(e)}")
            return None

class HonorsExtraction(ExtractionStrategy):
    def get_identifier(self):
        return "honors"

    def extract(self, driver):
        try:
            honors_container = WebDriverWait(driver, 15).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[data-view-name="profile-component-entity"]'))
            )
            logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(honors_container)}")
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§ÙØªÙ† Ú©Ø§Ù†ØªÛŒÙ†Ø±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ: {str(e)}", exc_info=True)
            return None

        results = []

        for container in honors_container:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ
                title = self._extract_with_fallback(
                    container,
                    [
                        'div.mr1.t-bold span[aria-hidden="true"]',
                        'div.t-bold span[aria-hidden="true"]:first-child'
                    ],
                    "Ø¹Ù†ÙˆØ§Ù†"
                )

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® Ùˆ Ø³Ø§Ø²Ù…Ø§Ù†
                date_org = self._extract_with_fallback(
                    container,
                    [
                        'span.t-14.t-normal span[aria-hidden="true"]',
                        'div.t-14.t-normal:has(> span) span[aria-hidden="true"]'
                    ],
                    "ØªØ§Ø±ÛŒØ®/Ø³Ø§Ø²Ù…Ø§Ù†"
                )

                # ØªÙÚ©ÛŒÚ© ØªØ§Ø±ÛŒØ® Ùˆ Ø³Ø§Ø²Ù…Ø§Ù†
                date, organization = self._split_date_org(date_org)

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª
                description = self._extract_with_fallback(
                    container,
                    [
                        'div.pvs-entity__sub-components span[aria-hidden="true"]:not(.visually-hidden)',
                        'div.ZrKmYbhRDKWCwyJWVTSwOKEpalqlqVLKjOM span[aria-hidden="true"]'
                    ],
                    "ØªÙˆØ¶ÛŒØ­Ø§Øª"
                )

                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§Ø²Ù…Ø§Ù† Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù„ÙˆÚ¯Ùˆ
                if not organization:
                    organization = self._extract_org_from_logo(container)

                if title:
                    results.append({
                        "title": title,
                        "date": date,
                        "organization": organization,
                        "description": description
                    })

            except Exception as e:
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù†ØªÛŒÙ†Ø±: {str(e)}", exc_info=True)
                continue

        return results

    def _extract_with_fallback(self, parent, selectors, field_name):
        for selector in selectors:
            try:
                element = parent.find_element(By.CSS_SELECTOR, selector)
                text = element.text.strip()
                if text:
                    return text
            except Exception as e:
                continue
            except Exception as e:
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ {field_name}: {str(e)}")
        return ""

    def _split_date_org(self, text):
        try:
            if "Â·" in text:
                parts = [p.strip() for p in text.split("Â·")]
                return (parts[0], parts[1]) if len(parts) > 1 else (parts[0], "")
            return (text, "")
        except Exception:
            return ("", "")

    def _extract_org_from_logo(self, container):
        try:
            org_element = container.find_element(
                By.CSS_SELECTOR,
                'li.ivm-entity-pile__img-item--stacked + div span[aria-hidden="true"]'
            )
            return org_element.text.strip()
        except Exception:
            return ""

class LanguagesExtraction(ExtractionStrategy):
    def get_identifier(self):
        return "languages"

    def extract(self, driver):
        try:
            language_containers = self._wait_for_language_containers(driver)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§ÙØªÙ† Ø¨Ø®Ø´ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§: {str(e)}", exc_info=True)
            return None

        try:
            languages = self._process_language_containers(language_containers)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§: {str(e)}", exc_info=True)
            return None

        logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(languages)}")
        return languages

    def _wait_for_language_containers(self, driver):
        return WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div[data-view-name="profile-component-entity"]'))
        )

    def _process_language_containers(self, containers):
        results = []
        for container in containers:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ù„Ú©ØªÙˆØ± Ø¯Ù‚ÛŒÙ‚ØªØ± Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù… Ø²Ø¨Ø§Ù†
            try:
                language_element = container.find_element(
                    By.CSS_SELECTOR,
                    "div.display-flex.align-items-center.mr1.t-bold span[aria-hidden='true']"
                )
                language = language_element.text.strip()
            except Exception as e:
                # Ø§Ú¯Ø± Ø§Ù„Ù…Ù†Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§ÛŒÙ† container Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ù†ÛŒØ³Øª
                continue
            except Exception as e :
                logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø²Ø¨Ø§Ù†: {str(e)}", exc_info=True)
                language = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø·Ø­ Ø²Ø¨Ø§Ù†
            try:
                level_element = container.find_element(
                    By.CSS_SELECTOR,
                    "span.pvs-entity__caption-wrapper[aria-hidden='true']"
                )
                level = level_element.text.strip()
            except Exception as e:
                level = ""

            if language:

                results.append({"language": language, "level": level})

        return results

class ProjectsExtraction(ExtractionStrategy):
    def extract(self, driver):
        projects_data = []

        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù…ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ù‡Ø³ØªÙ†Ø¯.
        project_components = driver.find_elements(By.XPATH, "//div[@data-view-name='profile-component-entity']")

        # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø§Ù„Ù…Ø§Ù†ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ ÛŒÚ© Ù„ÛŒØ³Øª Ø®Ø§Ù„ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒÙ….
        if not project_components:
            return {"projects_data": projects_data}

        # Ø­Ù„Ù‚Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± Ù¾Ø±ÙˆÚ˜Ù‡
        for project_component in project_components:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø±ÙˆÚ˜Ù‡ (Ù…Ø«Ù„Ø§Ù‹ "Mr. Musa Movie")
            try:
                title_element = project_component.find_element(By.CSS_SELECTOR, "div.t-bold span[aria-hidden='true']")
                project_title = title_element.text.strip()
            except Exception:
                project_title = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ (Ù…Ø«Ù„Ø§Ù‹ "Oct 2023 - Present")
            try:
                timeline_element = project_component.find_element(By.CSS_SELECTOR,
                                                                  "span.t-14.t-normal span[aria-hidden='true']")
                project_timeline = timeline_element.text.strip()
            except Exception:
                project_timeline = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾Ø±ÙˆÚ˜Ù‡
            try:
                description_element = project_component.find_element(By.CSS_SELECTOR,
                                                                     "div.t-14.t-normal.t-black span[aria-hidden='true']")
                project_description = description_element.text.strip()
            except Exception:
                project_description = ""

            projects_data.append({
                "title": project_title,
                "timeline": project_timeline,
                "description": project_description,
            })

        return projects_data

    def get_identifier(self):
        return "projects"

class VolunteeringExtraction(ExtractionStrategy):
    def extract(self, driver):
        volunteering_data = []

        # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Ø´ÛŒ WebDriverWait Ø¨Ù‡ Ù…Ø¯Øª 10 Ø«Ø§Ù†ÛŒÙ‡
        wait = WebDriverWait(driver, 10)

        try:
            # ØµØ¨Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø§ÙˆÙ„ÛŒÙ† Ø§Ù„Ù…Ø§Ù† ÙØ¹Ø§Ù„ÛŒØª ÙˆÙ„Ù†ØªØ±ÛŒ Ø¸Ø§Ù‡Ø± Ø´ÙˆØ¯.
            wait.until(EC.presence_of_element_located((By.XPATH, "//div[@data-view-name='profile-component-entity']")))
        except Exception as e:
            print("Timeout waiting for volunteering components:", e)
            return volunteering_data

        # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù…ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙˆÙ„Ù†ØªØ±ÛŒ
        volunteering_components = driver.find_elements(By.XPATH, "//div[@data-view-name='profile-component-entity']")

        for vol in volunteering_components:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø´/Ø¹Ù†ÙˆØ§Ù† ÙØ¹Ø§Ù„ÛŒØª (Role)
            try:
                role_element = vol.find_element(By.CSS_SELECTOR, "div.t-bold span[aria-hidden='true']")
                role = role_element.text.strip()
            except Exception:
                role = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø³Ø§Ø²Ù…Ø§Ù† (Organization)
            try:
                organization_element = vol.find_element(By.CSS_SELECTOR, "span.t-14.t-normal:not(.t-black--light)")
                organization = organization_element.text.strip()
            except Exception:
                organization = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ (Timeline) Ùˆ Ø­ÙˆØ²Ù‡ ÙØ¹Ø§Ù„ÛŒØª (Cause)
            timeline = ""
            cause = ""
            try:
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ú©Ù„Ø§Ø³ t-14 t-normal t-black--light Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯
                black_light_spans = vol.find_elements(By.CSS_SELECTOR, "span.t-14.t-normal.t-black--light")
                if black_light_spans:
                    # Ø§ÙˆÙ„ÛŒÙ† Ø§Ù„Ù…Ø§Ù† Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø³Øª.
                    try:
                        timeline_element = black_light_spans[0].find_element(By.CSS_SELECTOR,
                                                                             "span.pvs-entity__caption-wrapper")
                        timeline = timeline_element.text.strip()
                    except Exception:
                        timeline = black_light_spans[0].text.strip()

                    # Ø¯ÙˆÙ…ÛŒÙ† Ø§Ù„Ù…Ø§Ù† Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ØŒ Ø­ÙˆØ²Ù‡ ÛŒØ§ Ø¯Ù„ÛŒÙ„ ÙØ¹Ø§Ù„ÛŒØª Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
                    if len(black_light_spans) > 1:
                        cause = black_light_spans[1].text.strip()
            except Exception:
                pass

            volunteering_data.append({
                "role": role,
                "organization": organization,
                "timeline": timeline,
                "cause": cause,
            })

        return {"volunteering_data": volunteering_data}

    def get_identifier(self):
        return "volunteering"

class CertificationsExtraction(ExtractionStrategy):
    def extract(self, driver):
        certifications_data = []

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡ Ø¬Ù‡Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ lazy-loaded
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)  # ÙˆÙ‚ÙÙ‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡

        # Ø§Ù†ØªØ¸Ø§Ø± ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø§Ù„Ù…Ø§Ù† Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡ Ø¸Ø§Ù‡Ø± Ø´ÙˆØ¯
        wait = WebDriverWait(driver, 10)
        try:
            wait.until(EC.presence_of_element_located(
                (By.XPATH, "//div[@data-view-name='profile-component-entity']")
            ))
        except Exception as e:
            logging.error("Timeout waiting for certifications components: %s", e)
            return certifications_data

        # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù…ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡
        cert_elements = driver.find_elements(By.XPATH, "//div[@data-view-name='profile-component-entity']")

        for cert in cert_elements:
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§Ù„Ù…Ø§Ù† Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ More Profiles for you Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
            try:
                cert.find_element(By.XPATH, ".//a[@data-field='browsemap_card_click']")
                # Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø§ÛŒÙ† ØªÚ¯ØŒ Ø¨Ù„ÙˆÚ© Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯
                continue
            except NoSuchElementException:
                pass

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡
            try:
                title_element = cert.find_element(By.CSS_SELECTOR, "div.t-bold span[aria-hidden='true']")
                cert_title = title_element.text.strip()
            except Exception:
                cert_title = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø³Ø§Ø²Ù…Ø§Ù† ØµØ§Ø¯Ø±Ú©Ù†Ù†Ø¯Ù‡
            try:
                # Ø¯Ø± Ø§ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ØŒ Ø§ÙˆÙ„ÛŒÙ† Ø§Ù„Ù…Ø§Ù† Ø¨Ø§ Ú©Ù„Ø§Ø³ t-14 t-normal Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù†Ø§Ù… Ø³Ø§Ø²Ù…Ø§Ù† Ø§Ø³Øª.
                org_element = cert.find_element(By.CSS_SELECTOR, "span.t-14.t-normal")
                issuing_org = org_element.text.strip()
            except Exception:
                issuing_org = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® ØµØ¯ÙˆØ± Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡
            try:
                # ØªØ§Ø±ÛŒØ® ØµØ¯ÙˆØ± Ø¯Ø± Ø¯Ø§Ø®Ù„ span Ø¨Ø§ Ú©Ù„Ø§Ø³ t-14 t-normal t-black--light Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯
                issued_date_element = cert.find_element(By.CSS_SELECTOR,
                                                        "span.t-14.t-normal.t-black--light span.pvs-entity__caption-wrapper")
                issued_date = issued_date_element.text.strip()
            except Exception:
                issued_date = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÙˆÚ¯ÙˆÛŒ Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡
            try:
                logo_element = cert.find_element(By.CSS_SELECTOR, "div.ivm-view-attr__img-wrapper img")
                logo_url = logo_element.get_attribute("src")
            except Exception:
                logo_url = ""

            # Ø§ÙØ²ÙˆØ¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
            if cert_title:
                certifications_data.append({
                    "title": cert_title,
                    "issuing_organization": issuing_org,
                    "issued_date": issued_date,
                    "logo_url": logo_url,
                })

        return {"certifications_data": certifications_data}

    def get_identifier(self):
        return "certifications"

class CoursesExtraction(ExtractionStrategy):
    def extract(self, driver):
        courses_data = []

        # ØµØ¨Ø± ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ ØµÙØ­Ù‡ Ú©Ø§Ù…Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯
        WebDriverWait(driver, 10).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡ Ø¬Ù‡Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ lazy-loaded
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)  # ÙˆÙ‚ÙÙ‡ Ø¨Ù‡ Ù…Ø¯Øª Û² Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§

        # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø¸Ø§Ù‡Ø± Ø´Ø¯Ù† Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø§Ù„Ù…Ø§Ù† Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ
        wait = WebDriverWait(driver, 10)
        try:
            wait.until(EC.presence_of_element_located(
                (By.XPATH, "//div[@data-view-name='profile-component-entity']")
            ))
        except Exception as e:
            logging.error("Timeout waiting for courses components: %s", e)
            return {"courses_data": courses_data}

        # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù…ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ (courses)
        course_elements = driver.find_elements(By.XPATH, "//div[@data-view-name='profile-component-entity']")

        for course in course_elements:
            # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ Ø§Ù„Ù…Ø§Ù† Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ More Profiles for you Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
            try:
                course.find_element(By.XPATH, ".//a[@data-field='browsemap_card_click']")
                # Ø§Ú¯Ø± Ø¹Ù†ØµØ± ÙÙˆÙ‚ ÛŒØ§ÙØª Ø´Ø¯ØŒ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ Ø¨Ø®Ø´ More Profiles for you Ø§Ø³ØªØ› Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ù† Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ…
                continue
            except NoSuchElementException:
                pass

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø¯ÙˆØ±Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ
            try:
                title_element = course.find_element(By.CSS_SELECTOR, "div.t-bold span[aria-hidden='true']")
                course_title = title_element.text.strip()
            except Exception:
                course_title = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø³Ø§Ø²Ù…Ø§Ù† ÛŒØ§ Ù…Ø±Ø¬Ø¹ Ù…Ø±ØªØ¨Ø· (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
            try:
                associated_org_element = course.find_element(
                    By.CSS_SELECTOR, "div.t-14.t-normal.t-black span[aria-hidden='true']"
                )
                associated_org = associated_org_element.text.strip()
            except Exception:
                associated_org = ""

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÙˆÚ¯ÙˆÛŒ Ø¯ÙˆØ±Ù‡ (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
            try:
                logo_element = course.find_element(By.CSS_SELECTOR, "div.ivm-view-attr__img-wrapper img")
                logo_url = logo_element.get_attribute("src")
            except Exception:
                logo_url = ""

            courses_data.append({
                "title": course_title,
                "associated_organization": associated_org,
                "logo_url": logo_url,
            })

        return courses_data

    def get_identifier(self):
        return "courses"

class ExperienceExtraction(ExtractionStrategy):
    def extract(self, driver):
        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø±Ø¯Ù† ØªØ§ Ù¾Ø§ÛŒØ§Ù† ØµÙØ­Ù‡ Ø¬Ù‡Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ lazyâ€‘loaded
        self.scroll_to_end(driver)

        try:
            wait = WebDriverWait(driver, 10)
            # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù…ÛŒ Ø¨Ù„ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ØªØ¬Ø±Ø¨Ù‡ Ú©Ù‡ Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ ÙˆØ§Ù„Ø¯ÛŒÙ†ÛŒ Ø¨Ø§ Ú©Ù„Ø§Ø³ pvs-entity__sub-components ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
            outer_experiences = wait.until(
                EC.presence_of_all_elements_located((
                    By.XPATH,
                    "//div[@data-view-name='profile-component-entity' and not(ancestor::div[contains(@class, 'pvs-entity__sub-components')])]"
                ))
            )
        except TimeoutException:
            logging.error("Timeout waiting for outer experience containers.")
            return {"experience_data": []}

        experiences = []
        for outer in outer_experiences:
            # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø¨ØªØ¯Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ Ø¨Ù„ÙˆÚ© Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Â«More Profiles for youÂ» Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±
            try:
                outer.find_element(By.XPATH, ".//a[@data-field='browsemap_card_click']")
                # Ø§Ú¯Ø± Ø§ÛŒÙ† ØªÚ¯ ÛŒØ§ÙØª Ø´Ø¯ØŒ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ More Profiles Ø¨ÙˆØ¯Ù‡ Ùˆ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                continue
            except NoSuchElementException:
                pass

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª Ø§Ø² Ø¨Ù„ÙˆÚ© Ø§ØµÙ„ÛŒ
            company = ""
            try:
                company_elem = outer.find_element(
                    By.XPATH,
                    ".//div[contains(@class, 't-bold')]/span[@aria-hidden='true']"
                )
                company = company_elem.text.strip()
            except Exception as e:
                logging.debug("Company name not found: %s", e)

            company_duration = ""
            try:
                duration_elem = outer.find_element(
                    By.XPATH,
                    ".//span[contains(@class, 't-14 t-normal') and not(contains(@class, 't-black--light'))]//span[@aria-hidden='true']"
                )
                company_duration = duration_elem.text.strip()
            except Exception as e:
                logging.debug("Company duration not found: %s", e)

            location = ""
            try:
                loc_elem = outer.find_element(
                    By.XPATH,
                    ".//span[contains(@class, 't-14 t-normal t-black--light')]//span[@aria-hidden='true']"
                )
                location = loc_elem.text.strip()
            except Exception as e:
                logging.debug("Location not found: %s", e)

            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ (ØªÙˆØ¯Ø±ØªÙˆ) Ø¯Ø± Ø¨Ù„ÙˆÚ© Ø§ØµÙ„ÛŒ
            nested_positions = []
            try:
                sub_components = outer.find_element(
                    By.XPATH,
                    ".//div[contains(@class, 'pvs-entity__sub-components')]"
                )
                position_items = sub_components.find_elements(
                    By.XPATH,
                    ".//li[contains(@class, 'pvs-list__paged-list-item')]"
                )
                for pos in position_items:
                    title = ""
                    pos_duration = ""
                    description = ""
                    try:
                        title_elem = pos.find_element(
                            By.XPATH,
                            ".//div[contains(@class, 't-bold')]/span[@aria-hidden='true']"
                        )
                        title = title_elem.text.strip()
                    except Exception as e:
                        logging.debug("Position title not found: %s", e)

                    try:
                        duration_elem = pos.find_element(
                            By.XPATH,
                            ".//span[contains(@class, 't-14 t-normal t-black--light')]//span[@aria-hidden='true']"
                        )
                        pos_duration = duration_elem.text.strip()
                    except Exception as e:
                        logging.debug("Position duration not found: %s", e)

                    try:
                        # ØªÙˆØ¶ÛŒØ­Ø§Øª Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± ÛŒÚ© Ø¨Ø®Ø´ Ø¨Ø§ Ú©Ù„Ø§Ø³ "t-14 t-normal t-black" Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
                        desc_elem = pos.find_element(
                            By.XPATH,
                            ".//div[contains(@class, 't-14 t-normal t-black')]//span[@aria-hidden='true']"
                        )
                        description = desc_elem.text.strip()
                    except Exception as e:
                        logging.debug("Position description not found: %s", e)

                    nested_positions.append({
                        "title": title,
                        "duration": pos_duration,
                        "description": description
                    })
            except Exception as e:
                logging.debug("No nested positions found: %s", e)

            # Ø§Ú¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ ØªÙˆØ¯Ø±ØªÙˆ ÛŒØ§ÙØª Ø´Ø¯Ù†Ø¯ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ø¯Ø§Ù… ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            if nested_positions:
                for pos in nested_positions:
                    experiences.append({
                        "company": company,
                        "company_duration": company_duration,
                        "location": location,
                        "title": pos.get("title", ""),
                        "duration": pos.get("duration", ""),
                        "description": pos.get("description", "")
                    })
            else:
                # Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ØŒ Ø¨Ù„ÙˆÚ© Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© ØªØ¬Ø±Ø¨Ù‡ Ø«Ø¨Øª Ù…ÛŒâ€ŒØ´ÙˆØ¯.
                experiences.append({
                    "company": company,
                    "company_duration": company_duration,
                    "location": location,
                    "title": company,  # Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„Øª Ø¹Ù†ÙˆØ§Ù† Ù‡Ù…Ø§Ù† Ù†Ø§Ù… Ø´Ø±Ú©Øª Ø§Ø³Øª
                    "duration": company_duration,
                    "description": ""
                })

        return experiences

    def get_identifier(self):
        return "experience"

    def scroll_to_end(self, driver, max_retries=3, scroll_pause=1.5):
        """
        Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ ØªØ§ Ù¾Ø§ÛŒØ§Ù† Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ lazyâ€‘loaded
        """
        last_height = driver.execute_script("return document.body.scrollHeight")
        retries = 0
        while retries < max_retries:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                retries += 1
            else:
                retries = 0
                last_height = new_height

class OrganizationsExtraction(ExtractionStrategy):
    def extract(self, driver):
        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø±Ø¯Ù† ØªØ§ Ù¾Ø§ÛŒØ§Ù† ØµÙØ­Ù‡ Ø¬Ù‡Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ lazyâ€‘loaded
        self.scroll_to_end(driver)

        try:
            wait = WebDriverWait(driver, 10)
            # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù…ÛŒ Ø¨Ù„ÙˆÚ©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø§Ø±Ø§ÛŒ data-view-name="profile-component-entity" Ù‡Ø³ØªÙ†Ø¯
            org_blocks = wait.until(
                EC.presence_of_all_elements_located(
                    (By.XPATH, "//div[@data-view-name='profile-component-entity']")
                )
            )
        except TimeoutException:
            logging.error("Timeout waiting for organizations components.")
            return {"organizations_data": []}

        organizations = []
        for org in org_blocks:
            # Ø§Ú¯Ø± Ø¨Ù„ÙˆÚ© Ø­Ø§ÙˆÛŒ ØªÚ¯ <a data-field="browsemap_card_click"> Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            try:
                org.find_element(By.XPATH, ".//a[@data-field='browsemap_card_click']")
                # Ø¯Ø± ØµÙˆØ±Øª ÛŒØ§ÙØªÙ† Ø¹Ù†ØµØ± ÙÙˆÙ‚ØŒ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ Ø¨Ø®Ø´ More Profiles Ø§Ø³Øª
                continue
            except NoSuchElementException:
                pass

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø³Ø§Ø²Ù…Ø§Ù† Ø§Ø² Ø¯Ø§ÛŒÙˆÛŒ Ú©Ù‡ Ø­ØªÙ…Ø§Ù‹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
            org_name = ""
            try:
                name_elem = org.find_element(
                    By.XPATH, ".//div[contains(@class, 'display-flex') and contains(@class, 'align-items-center') and contains(@class, 'mr1') and contains(@class, 't-bold')]/span[@aria-hidden='true']"
                )
                org_name = name_elem.text.strip()
            except Exception as e:
                logging.debug("Organization name not found: %s", e)
                continue  # Ø§Ú¯Ø± Ù†Ø§Ù… Ø³Ø§Ø²Ù…Ø§Ù† ÛŒØ§ÙØª Ù†Ø´Ø¯ØŒ Ø§Ø¯Ø§Ù…Ù‡â€ŒÛŒ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´ÙˆØ¯

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù‚Ø´ Ùˆ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ (role & date)
            role_date = ""
            try:
                role_date_elem = org.find_element(
                    By.XPATH, ".//span[contains(@class, 't-14 t-normal')]/span[@aria-hidden='true']"
                )
                role_date = role_date_elem.text.strip()
            except Exception as e:
                logging.debug("Role and date not found: %s", e)

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ±â€ŒØ¨Ø®Ø´ (Ù„ÙˆÚ¯ÙˆØŒ association Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª)
            logo_url = ""
            association = ""
            description = ""
            try:
                sub_components = org.find_element(
                    By.XPATH, ".//div[contains(@class, 'pvs-entity__sub-components')]"
                )
                # Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù…ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø²ÛŒØ±â€ŒØ¨Ø®Ø´
                li_items = sub_components.find_elements(By.XPATH, ".//li")
                for li in li_items:
                    li_text = li.text.strip()
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÙˆÚ¯Ùˆ (Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¢ÛŒØªÙ… Ø´Ø§Ù…Ù„ ØªÚ¯ img Ø¨Ø§Ø´Ø¯)
                    try:
                        img = li.find_element(By.XPATH, ".//img")
                        logo_url = img.get_attribute("src")
                    except Exception:
                        pass
                    # Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªÙ† Ø¢ÛŒØªÙ…: Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ "Associated with" Ø¢Ù† Ø±Ø§ Ø¯Ø± association Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
                    if "Associated with" in li_text:
                        association = li_text
                    # Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ù…ØªÙ† Ø¨Ø§ Ø¹Ù„Ø§Ù…Øª "â€¢" Ø´Ø±ÙˆØ¹ Ø´ÙˆØ¯ØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
                    if li_text.startswith("â€¢"):
                        if description:
                            description += "\n" + li_text
                        else:
                            description = li_text
            except Exception as e:
                logging.debug("Sub-components not found for organization: %s", e)

            organizations.append({
                "organization_name": org_name,
                "role_date": role_date,
                "logo_url": logo_url,
                "association": association,
                "description": description
            })

        return organizations

    def get_identifier(self):
        return "organizations"

    def scroll_to_end(self, driver, max_retries=3, scroll_pause=1.5):
        """
        Ø§Ø³Ú©Ø±ÙˆÙ„ Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ ØªØ§ Ù¾Ø§ÛŒØ§Ù† Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ lazyâ€‘loaded
        """
        last_height = driver.execute_script("return document.body.scrollHeight")
        retries = 0
        while retries < max_retries:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(scroll_pause)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                retries += 1
            else:
                retries = 0
                last_height = new_height


# ======================================================================
# Ù…Ø±Ø­Ù„Ù‡ Û²: ØªÙ†Ø¸ÛŒÙ… ÙÚ©ØªÙˆØ±ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ (Ø¨Ø§ Û· Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±)
# ======================================================================

class ExtractionStrategyFactory:
    @staticmethod
    def get_strategy(url):
        if "recent-activity" in url:
            return ActivityExtraction()
        elif "details/education" in url:
            return EducationExtraction()
        elif "details/skills" in url:
            return SkillsExtraction()
        elif "details/publications" in url:
            return PublicationsExtraction()
        elif "details/honors" in url:
            return HonorsExtraction()
        elif "details/languages" in url:
            return LanguagesExtraction()
        elif "details/projects" in url:
            return ProjectsExtraction()
        elif "details/volunteering-experiences" in url:
            return VolunteeringExtraction()
        elif "details/certifications" in url:
            return CertificationsExtraction()
        elif "details/courses" in url:
            return CoursesExtraction()
        elif "details/experience" in url:
            return ExperienceExtraction()
        elif "details/organizations" in url:
            return OrganizationsExtraction()
        elif "in/" in url:
            return ProfileExtraction()
        else:
            return ProfileExtraction()


# =============================================================================
# Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª ØªØ¨â€ŒÙ‡Ø§ (TabHandler)
# =============================================================================
class TabHandler:
    def __init__(self, driver, handle, url, extraction_strategy):
        self.driver = driver
        self.handle = handle
        self.url = url
        self.extraction_strategy = extraction_strategy

    def switch_to(self):
        self.driver.switch_to.window(self.handle)

    def extract_data(self):
        try:
            self.switch_to()
            logging.info(f"ğŸ“Š Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ØªØ¨: {self.url}")
            data = {
                "url": self.url,
                "data": self.extraction_strategy.extract(self.driver),
                "strategy": self.extraction_strategy.get_identifier()
            }
            return data
        except Exception as e:
            logging.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² {self.url}: {str(e)}")
            return None
        finally:
            self._close_tab()

    def _close_tab(self):
        try:
            if self.handle in self.driver.window_handles:
                self.driver.switch_to.window(self.handle)
                self.driver.close()
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø³ØªÙ† ØªØ¨: {str(e)}")

# ---------------------------------------------------------------------
# Ú©Ù„Ø§Ø³ LinkedinAutomation Ø¬Ù‡Øª Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø±ÙˆØ±Ú¯Ø±ØŒ ÙˆØ±ÙˆØ¯ØŒ ØµÙ URLÙ‡Ø§ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬
# ---------------------------------------------------------------------
class LinkedinAutomation:
    def __init__(self, cookie_file="linkedin_cookies.json", headless=False):
        self.cookie_file = cookie_file
        self.headless = headless
        self.driver = None
        self.wait = None
        self.tab_queue = Queue()
        self.main_handle = None

    def configure_driver(self):
        try:
            options = uc.ChromeOptions()
            if self.headless:
                options.add_argument("--headless=new")
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1280,720")
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--mute-audio")
            options.add_argument("--disable-popup-blocking")
            options.add_argument(
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
            )
            driver = uc.Chrome(options=options)
            driver.set_page_load_timeout(45)
            driver.set_script_timeout(30)
            logging.info("âœ… Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø´Ø¯")
            return driver
        except Exception as e:
            logging.exception(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±: {str(e)}")
            raise RuntimeError(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±: {str(e)}")

    async def async_configure_driver(self):
        loop = asyncio.get_running_loop()
        self.driver = await loop.run_in_executor(None, self.configure_driver)
        self.wait = WebDriverWait(self.driver, 45 if self.headless else 25)
        self.main_handle = self.driver.current_window_handle

    def human_like_interaction(self, element=None):
        try:
            actions = ActionChains(self.driver)
            if element:
                actions.move_to_element(element).perform()
            else:
                x_offset = random.randint(-50, 50)
                y_offset = random.randint(-50, 50)
                actions.move_by_offset(x_offset, y_offset).perform()
            time.sleep(random.uniform(0.5, 3))
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø±ÙØªØ§Ø±: {str(e)}")

    def handle_cookies(self, action: str):
        try:
            if action == "load":
                self.driver.get("https://www.linkedin.com/")
                time.sleep(2)
                with open(self.cookie_file, 'r', encoding='utf-8') as file:
                    cookies = json.load(file)
                for cookie in cookies:
                    if 'sameSite' in cookie and cookie['sameSite'] not in ['Strict', 'Lax', 'None']:
                        cookie['sameSite'] = 'Lax'
                    if cookie.get("name", "").startswith("__Host-"):
                        cookie.pop("domain", None)
                    self.driver.add_cookie(cookie)
                logging.info("âœ… Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯.")
                return True
            elif action == "save":
                cookies = self.driver.get_cookies()
                with open(self.cookie_file, 'w', encoding='utf-8') as file:
                    json.dump(cookies, file, indent=2, ensure_ascii=False)
                logging.info("ğŸ”’ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
                return True
        except FileNotFoundError:
            logging.warning("âš ï¸ ÙØ§ÛŒÙ„ Ú©ÙˆÚ©ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return False
        except json.JSONDecodeError:
            logging.warning("âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ù…Øª ÙØ§ÛŒÙ„ Ú©ÙˆÚ©ÛŒ.")
            return False
        except Exception as e:
            logging.exception(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡: {str(e)}")
            return False

    def check_cloudflare_challenge(self):
        try:
            self.wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, "#cf-challenge-running, .ray_id, .attack-box")
            ))
            logging.info("â³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú†Ø§Ù„Ø´ Cloudflare...")
            for _ in range(5):
                self.human_like_interaction()
                time.sleep(random.uniform(2, 5))
            return True
        except TimeoutException:
            return False
        except Exception as e:
            logging.exception(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø¯ÛŒØ±ÛŒØª Ú†Ø§Ù„Ø´: {str(e)}")
            return False

    def login(self):
        try:
            self.driver.get("https://www.linkedin.com/")
            if self.check_cloudflare_challenge():
                logging.info("âœ… Ú†Ø§Ù„Ø´ Cloudflare Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø´Øª Ø³Ø± Ú¯Ø°Ø§Ø´ØªÙ‡ Ø´Ø¯.")
            if self.handle_cookies("load"):
                self.driver.refresh()
                if self.is_logged_in():
                    logging.info("âœ… ÙˆØ±ÙˆØ¯ Ø¨Ø§ Ú©ÙˆÚ©ÛŒ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø¨ÙˆØ¯.")
                    return
            if self.headless:
                logging.info("ğŸ”‘ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ú©Ø§Ø± Ù†Ú©Ø±Ø¯Ù†Ø¯. ØªØºÛŒÛŒØ± Ø­Ø§Ù„Øª headless Ø¨Ù‡ False Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ù…Ø±ÙˆØ±Ú¯Ø±.")
                self.headless = False
                self.shutdown()
                self.driver = self.configure_driver()
                self.wait = WebDriverWait(self.driver, 25)
            logging.info("ğŸ”‘ Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙˆØ±ÙˆØ¯ Ø¯Ø³ØªÛŒ Ø¯Ø§Ø±ÛŒØ¯.")
            self.driver.get("https://www.linkedin.com/login")
            input("Ù¾Ø³ Ø§Ø² ØªÚ©Ù…ÛŒÙ„ ÙˆØ±ÙˆØ¯ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø±ØŒ Enter Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯...")
            if self.is_logged_in():
                self.handle_cookies("save")
                logging.info("âœ… Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯.")
            else:
                logging.error("âš ï¸ ÙˆØ±ÙˆØ¯ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯. Ù„Ø·ÙØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ±ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
                raise Exception("ÙˆØ±ÙˆØ¯ Ù†Ø§Ù…ÙˆÙÙ‚.")
        except Exception as e:
            logging.exception(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± ÙˆØ±ÙˆØ¯: {str(e)}")
            raise

    async def async_login(self):
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self.login)

    def is_logged_in(self):
        try:
            self.wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, "nav[aria-label='Primary Navigation']")
            ))
            self.wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, "img.global-nav__me-photo")
            ))
            return True
        except TimeoutException:
            logging.warning("âš ï¸ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
            return False
        except Exception as e:
            logging.exception(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª ÙˆØ±ÙˆØ¯: {str(e)}")
            return False

    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ¨â€ŒÙ‡Ø§
    def add_urls_to_queue(self, urls):
        for url in urls:
            # ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ Ú©Ù‡ ExtractionStrategyFactory Ø§Ø² Ù‚Ø¨Ù„ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø±Ø¬Ø§Ø¹ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
            strategy = ExtractionStrategyFactory.get_strategy(url)
            self.tab_queue.put((url, strategy))
        logging.info(f"âœ… {self.tab_queue.qsize()} Ø¢Ø¯Ø±Ø³ Ø¨Ù‡ ØµÙ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù†Ø¯.")

    def check_target_in_database(self, connection_string, target):
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ØªØ§Ø±Ú¯Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
        try:
            conn = pyodbc.connect(connection_string)
            cursor = conn.cursor()
            query = "SELECT COUNT(*) FROM LinkedInUserData WHERE Target = ?"
            cursor.execute(query, (target,))
            count = cursor.fetchone()[0]
            return count > 0
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ØªØ§Ø±Ú¯Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}")
            return False
        finally:
            if conn:
                conn.close()

    def update_data_in_database(self, data, connection_string, target, update_fields=None):
        try:
            conn = pyodbc.connect(connection_string)
            cursor = conn.cursor()
            current_time = datetime.datetime.now()

            # Ø§Ú¯Ø± ØªÙ…Ø§Ù… ÙÛŒÙ„Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ Ø¢Ù¾Ø¯ÛŒØª Ø´ÙˆÙ†Ø¯
            if update_fields is None:
                update_fields = data.keys()

            set_clauses = []
            params = []
            for field in update_fields:
                field_key = field
                json_data = json.dumps(data.get(field_key, []), ensure_ascii=False)
                column_name = f"{field_key.capitalize()}Data"
                set_clauses.append(f"{column_name} = ?")
                params.append(json_data)

            set_clauses.append("UpdatedAt = ?")
            params.append(current_time)
            params.append(target)

            set_clause_str = ", ".join(set_clauses)
            query = f"""
                UPDATE [dbo].[LinkedInUserData]
                SET {set_clause_str}
                WHERE Target = ?
            """
            cursor.execute(query, params)
            conn.commit()
            logging.info("âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù†Ø¯.")
        except Exception as e:
            logging.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {str(e)}")
        finally:
            if conn:
                conn.close()

    def insert_data_into_database(self, data, connection_string, target):
        try:
            conn = pyodbc.connect(connection_string)
            cursor = conn.cursor()
            current_time = datetime.datetime.now()

            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ø³Ø±Øª
            profile_data = json.dumps(data.get("profile", []), ensure_ascii=False)
            activity_data = json.dumps(data.get("activity", []), ensure_ascii=False)
            education_data = json.dumps(data.get("education", []), ensure_ascii=False)
            skills_data = json.dumps(data.get("skills", []), ensure_ascii=False)
            publications_data = json.dumps(data.get("publications", []), ensure_ascii=False)
            honors_data = json.dumps(data.get("honors", []), ensure_ascii=False)
            languages_data = json.dumps(data.get("languages", []), ensure_ascii=False)
            projects_data = json.dumps(data.get("projects", []), ensure_ascii=False)
            volunteering_data = json.dumps(data.get("volunteering", []), ensure_ascii=False)
            certifications_data = json.dumps(data.get("certifications", []), ensure_ascii=False)
            courses_data = json.dumps(data.get("courses", []), ensure_ascii=False)
            experience_data = json.dumps(data.get("experience", []), ensure_ascii=False)
            organizations_data = json.dumps(data.get("organizations", []), ensure_ascii=False)

            query = """
                INSERT INTO [dbo].[LinkedInUserData] (
                    ProfileData, ActivityData, EducationData, SkillsData, PublicationsData,
                    HonorsData, LanguagesData, ProjectsData, VolunteeringData, CertificationsData,
                    CoursesData, ExperienceData, OrganizationsData, CreatedAt, UpdatedAt, Target
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """
            cursor.execute(query, (
                profile_data, activity_data, education_data, skills_data, publications_data,
                honors_data, languages_data, projects_data, volunteering_data, certifications_data,
                courses_data, experience_data, organizations_data, current_time, current_time, target
            ))
            conn.commit()
            logging.info("Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}")
        finally:
            if conn:
                conn.close()

    def process_queue(self, connection_string, target, update_mode=False, update_fields=None):
        results = {}
        while not self.tab_queue.empty():
            url, strategy = self.tab_queue.get()
            try:
                result = self._process_single_tab(url, strategy)
                if result:
                    results[result["strategy"]] = result["data"]
            except Exception as e:
                logging.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ¨ {url}: {str(e)}")
            finally:
                self._switch_to_main_window()
                self.tab_queue.task_done()

        if update_mode:
            self.update_data_in_database(results, connection_string, target, update_fields)
        else:
            self.insert_data_into_database(results, connection_string, target)
        return results

    def _process_single_tab(self, url, strategy):
        self._open_new_tab(url)
        new_handle = self._get_new_tab_handle()
        tab = TabHandler(self.driver, new_handle, url, strategy)
        return tab.extract_data()

    def _open_new_tab(self, url):
        self.driver.execute_script(f"window.open('{url}');")
        time.sleep(1)

    def _get_new_tab_handle(self):
        handles = self.driver.window_handles
        new_handles = [h for h in handles if h != self.main_handle]
        if new_handles:
            return new_handles[-1]
        else:
            raise Exception("Ù‡ÛŒÚ† ØªØ¨ Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")

    def _switch_to_main_window(self):
        try:
            self.driver.switch_to.window(self.main_handle)
        except Exception as e:
            logging.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù¾Ù†Ø¬Ø±Ù‡ Ø§ØµÙ„ÛŒ: {str(e)}")


    def shutdown(self):
        try:
            if self.driver:
                self.driver.quit()
                logging.info("Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ø¨Ø³ØªÙ‡ Ø´Ø¯.")
        except Exception as e:
            logging.exception(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø³ØªÙ† Ù…Ø±ÙˆØ±Ú¯Ø±: {str(e)}")

# ---------------------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ async
# ---------------------------------------------------------------------
# ---------------------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ async
# ---------------------------------------------------------------------



async def main():
    enabled_features = {
        "profile": True, "activity": True, "education": True, "skills": True,
        "publications": True, "honors": True, "languages": True, "projects": True,
        "volunteering": True, "certifications": True, "courses": True, "experience": True,
        "organizations": True
    }
    connection_string = database_project
    automation = LinkedinAutomation(cookie_file="linkedin_cookies.json", headless=False)

    try:
        await automation.async_configure_driver()
        await automation.async_login()

        target = input("Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒ Ù„ÛŒÙ†Ú©Ø¯ÛŒÙ† Ù‡Ø¯Ù Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ").strip()

        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ØªØ§Ø±Ú¯Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        target_exists = automation.check_target_in_database(connection_string, target)

        if target_exists:
            print("âœ… ØªØ§Ø±Ú¯Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª. Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ ØªØ­Ù„ÛŒÙ„...")

            # ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
            analyzer = LinkedInAnalyzer(target)
            try:
                # Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                data = analyzer.get_linkedin_data()

                # ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„
                print("\n" + "=" * 50 + " ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ " + "=" * 50)
                full_analysis = analyzer.analyze_full()
                print(full_analysis)

                # ØªØ­Ù„ÛŒÙ„ Ø¨Ø®Ø´ Ø¨Ù‡ Ø¨Ø®Ø´
                print("\n" + "=" * 50 + " ØªØ­Ù„ÛŒÙ„ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ " + "=" * 50)
                section_analysis = analyzer.analyze_sections_individually()
                for section, result in section_analysis.items():
                    print(f"\n{'-' * 30} {section.upper()} {'-' * 30}")
                    print(result)

            except RuntimeError as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {str(e)}")
                return

            # Ù¾Ø±Ø³Ø´ Ø¨Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            update_choice = input("\nØ¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯ØŸ (Ø¨Ù„Ù‡/Ø®ÛŒØ±): ").strip().lower()
            if update_choice == 'Ø¨Ù„Ù‡':
                # Ø¯Ø±ÛŒØ§ÙØª ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª
                print("\nÙ„Ø·ÙØ§Ù‹ Ø´Ù…Ø§Ø±Ù‡ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´ÙˆÙ†Ø¯ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ø¨Ø§ Ú©Ø§Ù…Ø§ Ø¬Ø¯Ø§ Ú©Ù†ÛŒØ¯):")
                fields = [
                    ("profile", "Ù¾Ø±ÙˆÙØ§ÛŒÙ„"), ("activity", "ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§"), ("education", "ØªØ­ØµÛŒÙ„Ø§Øª"),
                    ("skills", "Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§"), ("publications", "Ø§Ù†ØªØ´Ø§Ø±Ø§Øª"), ("honors", "Ø§ÙØªØ®Ø§Ø±Ø§Øª"),
                    ("languages", "Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§"), ("projects", "Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§"), ("volunteering", "Ø¯Ø§ÙˆØ·Ù„Ø¨Ø§Ù†Ù‡"),
                    ("certifications", "Ú¯ÙˆØ§Ù‡ÛŒÙ†Ø§Ù…Ù‡â€ŒÙ‡Ø§"), ("courses", "Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§"), ("experience", "ØªØ¬Ø±Ø¨Ù‡"),
                    ("organizations", "Ø³Ø§Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§")
                ]
                for idx, (key, name) in enumerate(fields, 1):
                    print(f"{idx}. {name}")
                selected = input("Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§: ").strip().split(',')
                update_fields = [fields[int(i.strip()) - 1][0] for i in selected if i.strip().isdigit()]

                # Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                base_url = f"https://www.linkedin.com/in/{target}/"
                urls_config = {
                    "profile": base_url, "activity": f"{base_url}recent-activity/all/",
                    "education": f"{base_url}details/education/", "skills": f"{base_url}details/skills/",
                    "publications": f"{base_url}details/publications/", "honors": f"{base_url}details/honors/",
                    "languages": f"{base_url}details/languages/", "projects": f"{base_url}details/projects/",
                    "volunteering": f"{base_url}details/volunteering-experiences/",
                    "certifications": f"{base_url}details/certifications/", "courses": f"{base_url}details/courses/",
                    "experience": f"{base_url}details/experience/", "organizations": f"{base_url}details/organizations/"
                }
                target_urls = [urls_config[key] for key in urls_config if key in update_fields]
                automation.add_urls_to_queue(target_urls)
                automation.process_queue(connection_string, target, update_mode=True, update_fields=update_fields)

                # ØªØ­Ù„ÛŒÙ„ Ù…Ø¬Ø¯Ø¯ Ù¾Ø³ Ø§Ø² Ø¢Ù¾Ø¯ÛŒØª
                print("\n" + "=" * 50 + " ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡ " + "=" * 50)
                try:
                    analyzer = LinkedInAnalyzer(target)
                    data = analyzer.get_linkedin_data()
                    full_analysis = analyzer.analyze_full()
                    print(full_analysis)
                except RuntimeError as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ² Ø´Ø¯Ù‡: {str(e)}")

        else:
            print("ğŸ” ØªØ§Ø±Ú¯Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ú©Ø±ÙˆÙ„...")

            # ØªÙ†Ø¸ÛŒÙ… URLÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„
            base_url = f"https://www.linkedin.com/in/{target}/"
            urls_config = {
                "profile": base_url, "activity": f"{base_url}recent-activity/all/",
                "education": f"{base_url}details/education/", "skills": f"{base_url}details/skills/",
                "publications": f"{base_url}details/publications/", "honors": f"{base_url}details/honors/",
                "languages": f"{base_url}details/languages/", "projects": f"{base_url}details/projects/",
                "volunteering": f"{base_url}details/volunteering-experiences/",
                "certifications": f"{base_url}details/certifications/", "courses": f"{base_url}details/courses/",
                "experience": f"{base_url}details/experience/", "organizations": f"{base_url}details/organizations/"
            }
            target_urls = [urls_config[key] for key in urls_config if enabled_features.get(key, False)]
            automation.add_urls_to_queue(target_urls)

            # Ø§Ù†Ø¬Ø§Ù… Ú©Ø±ÙˆÙ„ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            automation.process_queue(connection_string, target)

            # ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            print("\n" + "=" * 50 + " ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ " + "=" * 50)
            analyzer = LinkedInAnalyzer(target)
            try:
                data = analyzer.get_linkedin_data()
                full_analysis = analyzer.analyze_full()
                print(full_analysis)

                print("\n" + "=" * 50 + " ØªØ­Ù„ÛŒÙ„ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ " + "=" * 50)
                section_analysis = analyzer.analyze_sections_individually()
                for section, result in section_analysis.items():
                    print(f"\n{'-' * 30} {section.upper()} {'-' * 30}")
                    print(result)

            except RuntimeError as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯: {str(e)}")

    except Exception as e:
        logging.error(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡: {str(e)}")
    finally:
        automation.shutdown()


# ---------------------------------------------------------------------
# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§ØµÙ„ÛŒ
# ---------------------------------------------------------------------


if __name__ == '__main__':
    asyncio.run(main())


